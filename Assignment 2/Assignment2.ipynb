{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tghvrh-GwTE",
        "colab_type": "text"
      },
      "source": [
        "# DATA PREPROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXlh19Zq7G0",
        "colab_type": "code",
        "outputId": "838e3199-4060-4aca-9e0c-503b4c29f1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Both Texts are premodified to have only relevant content\n",
        "#non-ascii inverted commas converted to ascii types in text files\n",
        "#regex removes non-relevant symbols and chapter numbers\n",
        "\n",
        "import re\n",
        "import io\n",
        "import string\n",
        "\n",
        "path = \"\"\n",
        "\n",
        "with io.open(path+\"speeches.txt\",\"r\",encoding=\"utf-8\") as Speech:\n",
        "    Text = re.sub(\"SPEECH [0-9]+\",\"\",Speech.read())\n",
        "with io.open(path+\"Jane_Austen.txt\",\"r\",encoding=\"utf-8\") as JA:\n",
        "        Text+= re.sub(\"Chapter [0-9]+\",\"\",JA.read())\n",
        "Text = re.sub(\"[\\\"_,]\",\"\",Text)\n",
        "Text = re.sub(\"\\n\",\" \",Text)\n",
        "Text = Text.replace(\";\",\"\")\n",
        "Text = re.sub(\"--\",\"\",Text)\n",
        "Text = re.sub(\"(?:\\.\\.\\.)(?=[A-Z])\",\"\",Text)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIgVFJhKwOLy",
        "colab_type": "code",
        "outputId": "9341a3c7-4bf7-4625-aa8e-a59c9bf5bde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6HzhGKzG463",
        "colab_type": "text"
      },
      "source": [
        "# Classical Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2Y8ndWjkq7G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Sentence tokenisation\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent = sent_tokenize(Text.lower())\n",
        "\n",
        "#Processing sentences into list of words\n",
        "for i in range(len(sent)):\n",
        "  sent[i] = \" \".join(re.findall(\"#?[A-Za-z]+[\\.\\'?!-]*[A-Za-z]*[\\.\\'?!-]*\",sent[i]))\n",
        "  sent[i] = \"/ \" + sent[i][:-1] + \" \\\\\"\n",
        "\n",
        "#Train and Test data split\n",
        "s_train,s_test = train_test_split(sent,test_size=0.2,random_state=1)\n",
        "#Counting N-grams\n",
        "VT = set()\n",
        "UD = {} #uni\n",
        "BD = {} #bi\n",
        "TD = {} #tri\n",
        "QD = {} #quad\n",
        "TN=0\n",
        "for stl in sent:\n",
        "  st = stl.split()\n",
        "  TN+=len(st)\n",
        "  for j in range(len(st)):\n",
        "    VT.add(st[j])\n",
        "    if j+3<len(st):\n",
        "      if st[j] not in UD:\n",
        "        UD[st[j]] = 0\n",
        "      UD[st[j]]+=1\n",
        "      if st[j] + ' ' + st[j+1] not in BD:\n",
        "        BD[st[j] + ' ' + st[j+1]] = 0\n",
        "      BD[st[j] + ' ' + st[j+1]] += 1\n",
        "      if st[j] + ' ' + st[j+1] + ' ' + st[j+2] not in TD:\n",
        "        TD[st[j] + ' ' + st[j+1] + ' ' + st[j+2]] = 0\n",
        "      TD[st[j] + ' ' + st[j+1] + ' ' + st[j+2]] += 1\n",
        "      if st[j] + ' ' + st[j+1] + ' ' + st[j+2] + ' ' + st[j+3] not in QD:\n",
        "        QD[st[j] + ' ' + st[j+1] + ' ' + st[j+2] + ' ' + st[j+3]] = 0\n",
        "      QD[st[j] + ' ' + st[j+1] + ' ' + st[j+2] + ' ' + st[j+3]] += 1\n",
        "    elif j+2<len(st):\n",
        "      if st[j] not in UD:\n",
        "        UD[st[j]] = 0\n",
        "      UD[st[j]]+=1\n",
        "      if st[j] + ' ' + st[j+1] not in BD:\n",
        "        BD[st[j] + ' ' + st[j+1]] = 0\n",
        "      BD[st[j] + ' ' + st[j+1]] += 1\n",
        "      if st[j] + ' ' + st[j+1] + ' ' + st[j+2] not in TD:\n",
        "        TD[st[j] + ' ' + st[j+1] + ' ' + st[j+2]] = 0\n",
        "      TD[st[j] + ' ' + st[j+1] + ' ' + st[j+2]] += 1\n",
        "    elif j+1<len(st):\n",
        "      if st[j] not in UD:\n",
        "        UD[st[j]] = 0\n",
        "      UD[st[j]]+=1\n",
        "      if st[j] + ' ' + st[j+1] not in BD:\n",
        "        BD[st[j] + ' ' + st[j+1]] = 0\n",
        "      BD[st[j] + ' ' + st[j+1]] += 1\n",
        "    else:\n",
        "      if st[j] not in UD:\n",
        "        UD[st[j]] = 0\n",
        "      UD[st[j]]+=1\n",
        "V = len(VT)\n",
        "VT = sorted(list(VT-{\"/\"}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ACu4XhiHAfJ",
        "colab_type": "text"
      },
      "source": [
        "### Random Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzYSLxwV3hBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def nW(N,W,P):\n",
        "  return W[np.argmax(np.random.multinomial(10,P))]\n",
        "\n",
        "def genWPlist(given,UD,BD,TD,QD):\n",
        "  n = len(given) + 1\n",
        "  W = []\n",
        "  P = []\n",
        "  if n==2:\n",
        "    for bgrm in BD.keys():\n",
        "      Li = bgrm.split()\n",
        "      if Li[:-1] == given:\n",
        "        W.append(Li[1])  #appending the next word candidate\n",
        "        P.append(BD[bgrm]/UD[Li[0]]) #appending the next word candidate's MLE \n",
        "  if n==3:\n",
        "    for tgrm in TD.keys():\n",
        "      Li = tgrm.split()\n",
        "      if Li[:-1] == given:\n",
        "        W.append(Li[2]) #appending the next word candidate\n",
        "        P.append(TD[tgrm]/BD[Li[0]+' '+Li[1]]) #appending the next word candidate's MLE \n",
        "  if n==4:\n",
        "    for qgrm in QD.keys():\n",
        "      Li = qgrm.split()\n",
        "      if Li[:-1] == given:\n",
        "        W.append(Li[3]) #appending the next word candidate\n",
        "        P.append(QD[qgrm]/TD[Li[0]+' '+Li[1]+' '+Li[2]]) #appending the next word candidate's MLE \n",
        "  return W,P\n",
        "\n",
        "def Generator(N,UD,BD,TD,QD,TN,VT):\n",
        "  randtext = \"\"\n",
        "  for i in range(5):\n",
        "    rsent = [\"/\"]\n",
        "    if N==1:\n",
        "      W = VT[:]  #next word candidates\n",
        "      P = list(map(lambda x:UD[x]/TN,W[:])) #MLE of the next words in unigram model\n",
        "      while True:\n",
        "        rsent.append(W[np.argmax(np.random.multinomial(10,P))]) #appending the next word\n",
        "        if rsent == [\"/\",\"\\\\\"]:\n",
        "          rsent = [\"/\"]\n",
        "          continue\n",
        "        if rsent[-1]==\"\\\\\":\n",
        "          randtext += \" \".join(rsent[1:-1]) + \".\"\n",
        "          break  #stopping if next word was end of sentence\n",
        "    else:\n",
        "      while True:\n",
        "        given = rsent[max(0,len(rsent)-N+1):len(rsent)]\n",
        "        W,P = genWPlist(given,UD,BD,TD,QD) #generating next word candidates and MLE's of them\n",
        "        rsent.append(W[np.argmax(np.random.multinomial(10,P))]) #appending the next word\n",
        "        if rsent[-1]==\"\\\\\":\n",
        "          if len(rsent)<N:\n",
        "            continue\n",
        "          randtext += \" \".join(rsent[1:-1]) + \".\"\n",
        "          break  #stopping if next word was end of sentence\n",
        "  return randtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrKAdhe_uoC",
        "colab_type": "code",
        "outputId": "9d5dc6cc-890e-4704-a5b9-4dedc98aa7e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Unigram text generator\n",
        "print(Generator(1,UD,BD,TD,QD,TN,VT))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "agitation always the zones at.a and absurdity body's.conversation it greatly believing a.a at all but any followed are almost and as.zones and aware to and from but had had around of.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnsemejrJHFw",
        "colab_type": "code",
        "outputId": "1192131d-403f-4d42-e949-071c05cfc3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Bigram text generator\n",
        "print(Generator(2,UD,BD,TD,QD,TN,VT))"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "but he was with her husband.i shall not be.i am not been a lot of the first day and by the way.she was the country.i think of the room.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ7NZIDcS0Mr",
        "colab_type": "code",
        "outputId": "d7dbc50e-2a54-4602-fd2a-1f79ca6937a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Trigram text generator\n",
        "print(Generator(3,UD,BD,TD,QD,TN,VT))"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we have a problem that we shall be a very pretty shrubbery.but you know i'm going to happen.but i can only do so wish that he was a great job.they are to part with them.we're going to be very agreeable he had a far more than once.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm-yOAEDS0To",
        "colab_type": "code",
        "outputId": "ffadf8c9-bdd4-4c2a-e641-a514c0b4de7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Quadgram text generator, Note: Sometimes throws error due to non-occurence of some quadgram with a certain word.\n",
        "print(Generator(4,UD,BD,TD,QD,TN,VT))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he had been very little within that sum.he was a great deal to say in reply.that's what going to happen.and the reason i can say that i am not going to happen.and i have a great success you have to hit back hard and you can't solve the problem.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7hHjG6SC_yF",
        "colab_type": "text"
      },
      "source": [
        "#### The sentences get more readable at higher n gram models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtU_HPvHK-a",
        "colab_type": "text"
      },
      "source": [
        "### Perplexity Calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DtQt7WvTR-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Computing Perplexity on Test with add-1 smoothing\n",
        "from math import log,exp\n",
        "\n",
        "def cnt(st,N,UD,BD,TD,QD): #returns count of n grams\n",
        "  if N==1:\n",
        "    if st not in UD:\n",
        "      return 0\n",
        "    return UD[st]\n",
        "  elif N==2:\n",
        "    if st not in BD:\n",
        "      return 0\n",
        "    return BD[st]\n",
        "  elif N==3:\n",
        "    if st not in TD:\n",
        "      return 0\n",
        "    return TD[st]\n",
        "  elif N==4:\n",
        "    if st not in QD:\n",
        "      return 0\n",
        "    return QD[st]\n",
        "\n",
        "lpu = 0\n",
        "lpb = 0\n",
        "lpt = 0\n",
        "lpq = 0\n",
        "\n",
        "t = 0\n",
        "#computing log sum of MLE's with smoothing\n",
        "\n",
        "for ts in s_test:\n",
        "  lts = ts.split()\n",
        "  t+=1\n",
        "  for j in range(1,len(lts)-1):\n",
        "    if j-3>=0:\n",
        "      lpu+=log( ( cnt(lts[j],1,UD,BD,TD,QD) + 1 )/(TN + V) )\n",
        "      lpb+=log( ( cnt(lts[j-1]+' '+lts[j],2,UD,BD,TD,QD) + 1 )/( cnt(lts[j-1],1,UD,BD,TD,QD) + V ) )\n",
        "      lpt+=log( ( cnt(lts[j-2]+' '+lts[j-1]+' '+lts[j],3,UD,BD,TD,QD) + 1)/( cnt(lts[j-2]+' '+lts[j-1],2,UD,BD,TD,QD) + V ) )\n",
        "      lpq+=log( ( cnt(lts[j-3]+' '+lts[j-2]+' '+lts[j-1]+' '+lts[j],4,UD,BD,TD,QD) + 1 )/( cnt(lts[j-3]+' '+lts[j-2]+' '+lts[j-1],3,UD,BD,TD,QD) + V ) )\n",
        "    elif j-2>=0 and j-3<0: \n",
        "      lpu+=log( ( cnt(lts[j],1,UD,BD,TD,QD) + 1 )/( TN + V ) )\n",
        "      lpb+=log( ( cnt(lts[j-1]+' '+lts[j],2,UD,BD,TD,QD) + 1 )/( cnt(lts[j-1],1,UD,BD,TD,QD) + V ) )\n",
        "      lpt+=log( ( cnt(lts[j-2]+' '+lts[j-1]+' '+lts[j],3,UD,BD,TD,QD) + 1)/( cnt(lts[j-2]+' '+lts[j-1],2,UD,BD,TD,QD) + V ) )\n",
        "      lpq+=log( ( cnt(lts[j-2]+' '+lts[j-1]+' '+lts[j],3,UD,BD,TD,QD) + 1)/( cnt(lts[j-2]+' '+lts[j-1],2,UD,BD,TD,QD) + V ) )\n",
        "    elif j-1>=0 and j-2<0: \n",
        "      lpu+=log( ( cnt(lts[j],1,UD,BD,TD,QD) + 1 )/( TN + V ) )\n",
        "      lpb+=log( ( cnt(lts[j-1]+' '+lts[j],2,UD,BD,TD,QD) + 1 )/( cnt(lts[j-1],1,UD,BD,TD,QD) + V ) )\n",
        "      lpt+=log( ( cnt(lts[j-1]+' '+lts[j],2,UD,BD,TD,QD) + 1 )/( cnt(lts[j-1],1,UD,BD,TD,QD) + V ) )\n",
        "      lpq+=log( ( cnt(lts[j-1]+' '+lts[j],2,UD,BD,TD,QD) + 1 )/( cnt(lts[j-1],1,UD,BD,TD,QD) + V ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfV5s5NUjpWc",
        "colab_type": "code",
        "outputId": "6622de82-ca97-4b3a-f592-9a5b0e22f381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"Unigram Perplexity:\",exp(-lpu/t))\n",
        "print(\"Bigram Perplexity:\",exp(-lpb/t))\n",
        "print(\"Trigram Perplexity:\",exp(-lpt/t))\n",
        "print(\"Quadgram Perplexity:\",exp(-lpq/t))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 7.364168684167195e+51\n",
            "Bigram Perplexity: 1.0445888926731425e+55\n",
            "Trigram Perplexity: 1.095581907081848e+66\n",
            "Quadgram Perplexity: 1.0875556596948705e+69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqL8MWayHrcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}